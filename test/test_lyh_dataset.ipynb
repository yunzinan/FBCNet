{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this code is for used to play with the lyh dataset, for testing the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = \"../data/lyh/originalData\"\n",
    "left_v2_fp = \"left_processed_v2(300).npy\"\n",
    "left_v3_fp = \"left_processed_v3(500).npy\"\n",
    "right_v2_fp = \"right_processed_v2(300).npy\"\n",
    "right_v3_fp = \"right_processed_v3(500).npy\"\n",
    "leg_v2_fp = \"leg_processed_v2(300).npy\"\n",
    "leg_v3_fp = \"leg_processed_v3(500).npy\"\n",
    "nothing_v2_fp = \"nothing_processed_v2(300).npy\"\n",
    "nothing_v3_fp = \"nothing_processed_v3(500).npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # get all the data first\n",
    "        left_v2 = np.load(os.path.join(dir_path, left_v2_fp))\n",
    "        left_v3 = np.load(os.path.join(dir_path, left_v3_fp))\n",
    "        right_v2 = np.load(os.path.join(dir_path, right_v2_fp))\n",
    "        right_v3 = np.load(os.path.join(dir_path, right_v3_fp))\n",
    "        leg_v2 = np.load(os.path.join(dir_path, leg_v2_fp))\n",
    "        leg_v3 = np.load(os.path.join(dir_path, leg_v3_fp))\n",
    "        nothing_v2 = np.load(os.path.join(dir_path, nothing_v2_fp))\n",
    "        nothing_v3 = np.load(os.path.join(dir_path, nothing_v3_fp))\n",
    "        eeg_raw_v2 = [left_v2, right_v2, leg_v2, nothing_v2]\n",
    "        eeg_raw_v3 = [left_v3, right_v3, leg_v3, nothing_v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, (15, 500000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_raw_v2[0].shape[1], eeg_raw_v3[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseLyhFile(sessionId, epochWindow = [0,4], chans = list(range(22))):    \n",
    "    # X_tot = []\n",
    "    # y_tot = []\n",
    "\n",
    "    dir_path = \"../data/lyh/originalData\"\n",
    "    left_v2_fp = \"left_processed_v2(300).npy\"\n",
    "    left_v3_fp = \"left_processed_v3(500).npy\"\n",
    "    right_v2_fp = \"right_processed_v2(300).npy\"\n",
    "    right_v3_fp = \"right_processed_v3(500).npy\"\n",
    "    leg_v2_fp = \"leg_processed_v2(300).npy\"\n",
    "    leg_v3_fp = \"leg_processed_v3(500).npy\"\n",
    "    nothing_v2_fp = \"nothing_processed_v2(300).npy\"\n",
    "    nothing_v3_fp = \"nothing_processed_v3(500).npy\"\n",
    "    \n",
    "    # get all the data first\n",
    "    left_v2 = np.load(os.path.join(dir_path, left_v2_fp))\n",
    "    left_v3 = np.load(os.path.join(dir_path, left_v3_fp))\n",
    "    right_v2 = np.load(os.path.join(dir_path, right_v2_fp))\n",
    "    right_v3 = np.load(os.path.join(dir_path, right_v3_fp))\n",
    "    leg_v2 = np.load(os.path.join(dir_path, leg_v2_fp))\n",
    "    leg_v3 = np.load(os.path.join(dir_path, leg_v3_fp))\n",
    "    nothing_v2 = np.load(os.path.join(dir_path, nothing_v2_fp))\n",
    "    nothing_v3 = np.load(os.path.join(dir_path, nothing_v3_fp))\n",
    "    eeg_raw_v2 = [left_v2, right_v2, leg_v2, nothing_v2]\n",
    "    eeg_raw_v3 = [left_v3, right_v3, leg_v3, nothing_v3]\n",
    "\n",
    "    fs = 250\n",
    "    X_train_tot = []\n",
    "    X_test_tot = []\n",
    "    y_train_tot = []\n",
    "    y_test_tot = []\n",
    "    train_ratio = 0.8 # so that 80% trainset, the rest testset\n",
    "    eeg_raw = eeg_raw_v2 if sessionId == \"v2\" else eeg_raw_v3\n",
    "\n",
    "    for i in range(4):\n",
    "        # XXX: fixed the bug that you cannot simply reshape the files\n",
    "        # tmp = eeg_raw[i].reshape(15, 300, -1) # (15, 30_0000) => (15, 300, 1000)\n",
    "        # goal: (15, 30_0000) => (15, 300, 1000)\n",
    "        trial_list = []\n",
    "        n_trial = eeg_raw[0].shape[1] // 1000 # number of trials in the npy file\n",
    "        print(f\"load {n_trial} trials in the file.\")\n",
    "        for idx in range(n_trial):\n",
    "            trial_list.append(eeg_raw[i][:, idx * 1000:(idx + 1) * 1000]) # [1000:2000]\n",
    "        # now we have of a list of len 300, w/ each of shape (15, 1000)\n",
    "        tmp = np.stack(trial_list) # should give a shape of (300, 15, 1000)\n",
    "        X_raw = tmp[:, :14, :] # filter the channels, only need the first 14 channels\n",
    "        # (300, 14, 1000)\n",
    "        y_raw = np.array([i for j in range(n_trial)]) # (300,) value = label\n",
    "        # now shuffle the 300 samples \n",
    "        shuffle_idx = np.random.permutation(len(X_raw))\n",
    "        X_raw = X_raw[shuffle_idx, :, :]\n",
    "        y_raw = y_raw[shuffle_idx] # although no changes will be made\n",
    "        # X_tot.append(X_raw)\n",
    "        # y_tot.append(y_raw)\n",
    "        split_idx = int(n_trial * train_ratio)\n",
    "        X_train_tot.append(X_raw[:split_idx])\n",
    "        X_test_tot.append(X_raw[split_idx:])\n",
    "        y_train_tot.append(y_raw[:split_idx])\n",
    "        y_test_tot.append(y_raw[split_idx:])\n",
    "\n",
    "\n",
    "    # X_tot = np.concatenate(X_tot)\n",
    "    # y_tot = np.concatenate(y_tot)\n",
    "    X_train_tot = np.concatenate(X_train_tot) # (960, 14, 1000)\n",
    "    X_test_tot = np.concatenate(X_test_tot) # (240, 14, 1000)\n",
    "    y_train_tot = np.concatenate(y_train_tot) # (960,)\n",
    "    y_test_tot = np.concatenate(y_test_tot) # (240,)\n",
    "\n",
    "    # train_data = X_tot # (1200, 14, 1000)\n",
    "    # train_label = y_tot.reshape(1200, 1) # (1200, 1)\n",
    "        \n",
    "    # allData = train_data # (1200, 14, 1000)\n",
    "    # allLabel = train_label.squeeze() # (1200, )\n",
    "\n",
    "    shuffle_num = np.random.permutation(len(X_train_tot))\n",
    "    X_train = X_train_tot[shuffle_num, :, :]\n",
    "    y_train = y_train_tot[shuffle_num]\n",
    "    shuffle_num = np.random.permutation(len(X_test_tot))\n",
    "    X_test = X_test_tot[shuffle_num, :, :]\n",
    "    y_test = y_test_tot[shuffle_num]\n",
    "    # print(f\"Shuffle num {shuffle_num}\")\n",
    "    # allData = allData[shuffle_num, :, :]\n",
    "    # allLabel = allLabel[shuffle_num]\n",
    "\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(allData, allLabel, train_size=0.8,\n",
    "    #                                                             random_state=None, shuffle=False)\n",
    "\n",
    "    # now transpose the dimension to (n_chans, n_times, n_trial)\n",
    "    # allData = allData.transpose((1, 2, 0))\n",
    "    X_train = X_train.transpose((1, 2, 0))\n",
    "    X_test = X_test.transpose((1, 2, 0))\n",
    "    \n",
    "    # TODO: here, I just put the channels info to None, needs further configuration\n",
    "    # data = {'x': allData, 'y': allLabel, 'c': None, 's': fs}\n",
    "    train_data = {'x': X_train, 'y': y_train, 'c': [i for i in range(14)], 's': fs}\n",
    "    test_data = {'x': X_test, 'y': y_test, 'c': [i for i in range(14)], 's': fs}\n",
    "    #(n_chan, 1000, n_trials)\n",
    "    return train_data, test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 300 trials in the file.\n",
      "load 300 trials in the file.\n",
      "load 300 trials in the file.\n",
      "load 300 trials in the file.\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = parseLyhFile(sessionId=\"v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 500 trials in the file.\n",
      "load 500 trials in the file.\n",
      "load 500 trials in the file.\n",
      "load 500 trials in the file.\n"
     ]
    }
   ],
   "source": [
    "train_data_1, test_data_1 = parseLyhFile(sessionId=\"v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14, 1000, 960), (14, 1000, 240))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['x'].shape, test_data['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14, 1000, 1600), (14, 1000, 400))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_1['x'].shape, test_data_1['x'].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fbc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
